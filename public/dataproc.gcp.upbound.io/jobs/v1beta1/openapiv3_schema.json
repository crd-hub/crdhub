{
  "$schema": "http://json-schema.org/schema#",
  "description": "Job is the Schema for the Jobs API. Manages a job resource within a Dataproc cluster.",
  "type": "object",
  "required": [
    "spec"
  ],
  "properties": {
    "apiVersion": {
      "description": "APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources",
      "type": "string",
      "enum": [
        "dataproc.gcp.upbound.io/v1beta1"
      ]
    },
    "kind": {
      "description": "Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds",
      "type": "string",
      "enum": [
        "Job"
      ]
    },
    "metadata": {
      "type": "object",
      "properties": {
        "annotations": {
          "description": "Annotations is an unstructured key value map stored with a resource that may be set by external tools to store and retrieve arbitrary metadata. They are not queryable and should be preserved when modifying objects. More info: http://kubernetes.io/docs/user-guide/annotations",
          "type": "object",
          "additionalProperties": {
            "type": "string"
          }
        },
        "labels": {
          "description": "Map of string keys and values that can be used to organize and categorize (scope and select) objects. May match selectors of replication controllers and services. More info: http://kubernetes.io/docs/user-guide/labels",
          "type": "object",
          "additionalProperties": {
            "type": "string"
          }
        },
        "name": {
          "description": "Name must be unique within a namespace. Is required when creating resources, although some resources may allow a client to request the generation of an appropriate name automatically. Name is primarily intended for creation idempotence and configuration definition. Cannot be updated. More info: http://kubernetes.io/docs/user-guide/identifiers#names",
          "type": "string"
        }
      }
    },
    "spec": {
      "description": "JobSpec defines the desired state of Job",
      "type": "object",
      "required": [
        "forProvider"
      ],
      "properties": {
        "deletionPolicy": {
          "description": "DeletionPolicy specifies what will happen to the underlying external when this managed resource is deleted - either \"Delete\" or \"Orphan\" the external resource. This field is planned to be deprecated in favor of the ManagementPolicies field in a future release. Currently, both could be set independently and non-default values would be honored if the feature flag is enabled. See the design doc for more information: https://github.com/crossplane/crossplane/blob/499895a25d1a1a0ba1604944ef98ac7a1a71f197/design/design-doc-observe-only-resources.md?plain=1#L223",
          "type": "string",
          "default": "Delete",
          "enum": [
            "Orphan",
            "Delete"
          ]
        },
        "forProvider": {
          "type": "object",
          "properties": {
            "forceDelete": {
              "description": "By default, you can only delete inactive jobs within Dataproc. Setting this to true, and calling destroy, will ensure that the job is first cancelled before issuing the delete.",
              "type": "boolean"
            },
            "hadoopConfig": {
              "type": "array",
              "items": {
                "type": "object",
                "properties": {
                  "archiveUris": {
                    "description": "HCFS URIs of archives to be extracted in the working directory of .jar, .tar, .tar.gz, .tgz, and .zip.",
                    "type": "array",
                    "items": {
                      "type": "string"
                    }
                  },
                  "args": {
                    "description": "The arguments to pass to the driver. Do not include arguments, such as -libjars or -Dfoo=bar, that can be set as job properties, since a collision may occur that causes an incorrect job submission.",
                    "type": "array",
                    "items": {
                      "type": "string"
                    }
                  },
                  "fileUris": {
                    "description": "HCFS URIs of files to be copied to the working directory of Hadoop drivers and distributed tasks. Useful for naively parallel tasks.",
                    "type": "array",
                    "items": {
                      "type": "string"
                    }
                  },
                  "jarFileUris": {
                    "description": "HCFS URIs of jar files to add to the CLASSPATHs of the Spark driver and tasks.",
                    "type": "array",
                    "items": {
                      "type": "string"
                    }
                  },
                  "loggingConfig": {
                    "type": "array",
                    "items": {
                      "type": "object",
                      "properties": {
                        "driverLogLevels": {
                          "description": "The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'",
                          "type": "object",
                          "additionalProperties": {
                            "type": "string"
                          }
                        }
                      }
                    }
                  },
                  "mainClass": {
                    "description": "The name of the driver's main class. The jar file containing the class must be in the default CLASSPATH or specified in jar_file_uris. Conflicts with main_jar_file_uri",
                    "type": "string"
                  },
                  "mainJarFileUri": {
                    "description": "The HCFS URI of the jar file containing the main class. Examples: 'gs://foo-bucket/analytics-binaries/extract-useful-metrics-mr.jar' 'hdfs:/tmp/test-samples/custom-wordcount.jar' 'file:///home/usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar'. Conflicts with main_class",
                    "type": "string"
                  },
                  "properties": {
                    "description": "A mapping of property names to values, used to configure Hadoop. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site and classes in user code..",
                    "type": "object",
                    "additionalProperties": {
                      "type": "string"
                    }
                  }
                }
              }
            },
            "hiveConfig": {
              "type": "array",
              "items": {
                "type": "object",
                "properties": {
                  "continueOnFailure": {
                    "description": "Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries. Defaults to false.",
                    "type": "boolean"
                  },
                  "jarFileUris": {
                    "description": "HCFS URIs of jar files to add to the CLASSPATH of the Hive server and Hadoop MapReduce (MR) tasks. Can contain Hive SerDes and UDFs.",
                    "type": "array",
                    "items": {
                      "type": "string"
                    }
                  },
                  "properties": {
                    "description": "A mapping of property names and values, used to configure Hive. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml, /etc/hive/conf/hive-site.xml, and classes in user code..",
                    "type": "object",
                    "additionalProperties": {
                      "type": "string"
                    }
                  },
                  "queryFileUri": {
                    "description": "HCFS URI of file containing Hive script to execute as the job. Conflicts with query_list",
                    "type": "string"
                  },
                  "queryList": {
                    "description": "The list of Hive queries or statements to execute as part of the job. Conflicts with query_file_uri",
                    "type": "array",
                    "items": {
                      "type": "string"
                    }
                  },
                  "scriptVariables": {
                    "description": "Mapping of query variable names to values (equivalent to the Hive command: SET name=\"value\";).",
                    "type": "object",
                    "additionalProperties": {
                      "type": "string"
                    }
                  }
                }
              }
            },
            "labels": {
              "description": "The list of labels (key/value pairs) to add to the job.",
              "type": "object",
              "additionalProperties": {
                "type": "string"
              }
            },
            "pigConfig": {
              "type": "array",
              "items": {
                "type": "object",
                "properties": {
                  "continueOnFailure": {
                    "description": "Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries. Defaults to false.",
                    "type": "boolean"
                  },
                  "jarFileUris": {
                    "description": "HCFS URIs of jar files to add to the CLASSPATH of the Pig Client and Hadoop MapReduce (MR) tasks. Can contain Pig UDFs.",
                    "type": "array",
                    "items": {
                      "type": "string"
                    }
                  },
                  "loggingConfig": {
                    "type": "array",
                    "items": {
                      "type": "object",
                      "properties": {
                        "driverLogLevels": {
                          "description": "The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'",
                          "type": "object",
                          "additionalProperties": {
                            "type": "string"
                          }
                        }
                      }
                    }
                  },
                  "properties": {
                    "description": "A mapping of property names to values, used to configure Pig. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml, /etc/pig/conf/pig.properties, and classes in user code.",
                    "type": "object",
                    "additionalProperties": {
                      "type": "string"
                    }
                  },
                  "queryFileUri": {
                    "description": "HCFS URI of file containing Hive script to execute as the job. Conflicts with query_list",
                    "type": "string"
                  },
                  "queryList": {
                    "description": "The list of Hive queries or statements to execute as part of the job. Conflicts with query_file_uri",
                    "type": "array",
                    "items": {
                      "type": "string"
                    }
                  },
                  "scriptVariables": {
                    "description": "Mapping of query variable names to values (equivalent to the Pig command: name=[value]).",
                    "type": "object",
                    "additionalProperties": {
                      "type": "string"
                    }
                  }
                }
              }
            },
            "placement": {
              "type": "array",
              "items": {
                "type": "object",
                "properties": {
                  "clusterName": {
                    "description": "The name of the cluster where the job will be submitted.",
                    "type": "string"
                  },
                  "clusterNameRef": {
                    "description": "Reference to a Cluster in dataproc to populate clusterName.",
                    "type": "object",
                    "required": [
                      "name"
                    ],
                    "properties": {
                      "name": {
                        "description": "Name of the referenced object.",
                        "type": "string"
                      },
                      "policy": {
                        "description": "Policies for referencing.",
                        "type": "object",
                        "properties": {
                          "resolution": {
                            "description": "Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.",
                            "type": "string",
                            "default": "Required",
                            "enum": [
                              "Required",
                              "Optional"
                            ]
                          },
                          "resolve": {
                            "description": "Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.",
                            "type": "string",
                            "enum": [
                              "Always",
                              "IfNotPresent"
                            ]
                          }
                        }
                      }
                    }
                  },
                  "clusterNameSelector": {
                    "description": "Selector for a Cluster in dataproc to populate clusterName.",
                    "type": "object",
                    "properties": {
                      "matchControllerRef": {
                        "description": "MatchControllerRef ensures an object with the same controller reference as the selecting object is selected.",
                        "type": "boolean"
                      },
                      "matchLabels": {
                        "description": "MatchLabels ensures an object with matching labels is selected.",
                        "type": "object",
                        "additionalProperties": {
                          "type": "string"
                        }
                      },
                      "policy": {
                        "description": "Policies for selection.",
                        "type": "object",
                        "properties": {
                          "resolution": {
                            "description": "Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.",
                            "type": "string",
                            "default": "Required",
                            "enum": [
                              "Required",
                              "Optional"
                            ]
                          },
                          "resolve": {
                            "description": "Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.",
                            "type": "string",
                            "enum": [
                              "Always",
                              "IfNotPresent"
                            ]
                          }
                        }
                      }
                    }
                  }
                }
              }
            },
            "prestoConfig": {
              "type": "array",
              "items": {
                "type": "object",
                "properties": {
                  "clientTags": {
                    "description": "Presto client tags to attach to this query.",
                    "type": "array",
                    "items": {
                      "type": "string"
                    }
                  },
                  "continueOnFailure": {
                    "description": "Whether to continue executing queries if a query fails. Setting to true can be useful when executing independent parallel queries. Defaults to false.",
                    "type": "boolean"
                  },
                  "loggingConfig": {
                    "type": "array",
                    "items": {
                      "type": "object",
                      "properties": {
                        "driverLogLevels": {
                          "description": "The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'",
                          "type": "object",
                          "additionalProperties": {
                            "type": "string"
                          }
                        }
                      }
                    }
                  },
                  "outputFormat": {
                    "description": "The format in which query output will be displayed. See the Presto documentation for supported output formats.",
                    "type": "string"
                  },
                  "properties": {
                    "description": "A mapping of property names to values. Used to set Presto session properties Equivalent to using the --session flag in the Presto CLI.",
                    "type": "object",
                    "additionalProperties": {
                      "type": "string"
                    }
                  },
                  "queryFileUri": {
                    "description": "The HCFS URI of the script that contains SQL queries. Conflicts with query_list",
                    "type": "string"
                  },
                  "queryList": {
                    "description": "The list of SQL queries or statements to execute as part of the job. Conflicts with query_file_uri",
                    "type": "array",
                    "items": {
                      "type": "string"
                    }
                  }
                }
              }
            },
            "project": {
              "description": "The project in which the cluster can be found and jobs subsequently run against. If it is not provided, the provider project is used.",
              "type": "string"
            },
            "pysparkConfig": {
              "type": "array",
              "items": {
                "type": "object",
                "properties": {
                  "archiveUris": {
                    "description": "HCFS URIs of archives to be extracted in the working directory of .jar, .tar, .tar.gz, .tgz, and .zip.",
                    "type": "array",
                    "items": {
                      "type": "string"
                    }
                  },
                  "args": {
                    "description": "The arguments to pass to the driver.",
                    "type": "array",
                    "items": {
                      "type": "string"
                    }
                  },
                  "fileUris": {
                    "description": "HCFS URIs of files to be copied to the working directory of Python drivers and distributed tasks. Useful for naively parallel tasks.",
                    "type": "array",
                    "items": {
                      "type": "string"
                    }
                  },
                  "jarFileUris": {
                    "description": "HCFS URIs of jar files to add to the CLASSPATHs of the Python driver and tasks.",
                    "type": "array",
                    "items": {
                      "type": "string"
                    }
                  },
                  "loggingConfig": {
                    "type": "array",
                    "items": {
                      "type": "object",
                      "properties": {
                        "driverLogLevels": {
                          "description": "The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'",
                          "type": "object",
                          "additionalProperties": {
                            "type": "string"
                          }
                        }
                      }
                    }
                  },
                  "mainPythonFileUri": {
                    "description": "The HCFS URI of the main Python file to use as the driver. Must be a .py file.",
                    "type": "string"
                  },
                  "properties": {
                    "description": "A mapping of property names to values, used to configure PySpark. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.",
                    "type": "object",
                    "additionalProperties": {
                      "type": "string"
                    }
                  },
                  "pythonFileUris": {
                    "description": "HCFS file URIs of Python files to pass to the PySpark framework. Supported file types: .py, .egg, and .zip.",
                    "type": "array",
                    "items": {
                      "type": "string"
                    }
                  }
                }
              }
            },
            "reference": {
              "type": "array",
              "items": {
                "type": "object",
                "properties": {
                  "jobId": {
                    "type": "string"
                  }
                }
              }
            },
            "region": {
              "description": "The Cloud Dataproc region. This essentially determines which clusters are available for this job to be submitted to. If not specified, defaults to global.",
              "type": "string"
            },
            "regionRef": {
              "description": "Reference to a Cluster in dataproc to populate region.",
              "type": "object",
              "required": [
                "name"
              ],
              "properties": {
                "name": {
                  "description": "Name of the referenced object.",
                  "type": "string"
                },
                "policy": {
                  "description": "Policies for referencing.",
                  "type": "object",
                  "properties": {
                    "resolution": {
                      "description": "Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.",
                      "type": "string",
                      "default": "Required",
                      "enum": [
                        "Required",
                        "Optional"
                      ]
                    },
                    "resolve": {
                      "description": "Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.",
                      "type": "string",
                      "enum": [
                        "Always",
                        "IfNotPresent"
                      ]
                    }
                  }
                }
              }
            },
            "regionSelector": {
              "description": "Selector for a Cluster in dataproc to populate region.",
              "type": "object",
              "properties": {
                "matchControllerRef": {
                  "description": "MatchControllerRef ensures an object with the same controller reference as the selecting object is selected.",
                  "type": "boolean"
                },
                "matchLabels": {
                  "description": "MatchLabels ensures an object with matching labels is selected.",
                  "type": "object",
                  "additionalProperties": {
                    "type": "string"
                  }
                },
                "policy": {
                  "description": "Policies for selection.",
                  "type": "object",
                  "properties": {
                    "resolution": {
                      "description": "Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.",
                      "type": "string",
                      "default": "Required",
                      "enum": [
                        "Required",
                        "Optional"
                      ]
                    },
                    "resolve": {
                      "description": "Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.",
                      "type": "string",
                      "enum": [
                        "Always",
                        "IfNotPresent"
                      ]
                    }
                  }
                }
              }
            },
            "scheduling": {
              "type": "array",
              "items": {
                "type": "object",
                "properties": {
                  "maxFailuresPerHour": {
                    "description": "Maximum number of times per hour a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed.",
                    "type": "number"
                  },
                  "maxFailuresTotal": {
                    "description": "Maximum number of times in total a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed.",
                    "type": "number"
                  }
                }
              }
            },
            "sparkConfig": {
              "type": "array",
              "items": {
                "type": "object",
                "properties": {
                  "archiveUris": {
                    "description": "HCFS URIs of archives to be extracted in the working directory of .jar, .tar, .tar.gz, .tgz, and .zip.",
                    "type": "array",
                    "items": {
                      "type": "string"
                    }
                  },
                  "args": {
                    "description": "The arguments to pass to the driver.",
                    "type": "array",
                    "items": {
                      "type": "string"
                    }
                  },
                  "fileUris": {
                    "description": "HCFS URIs of files to be copied to the working directory of Spark drivers and distributed tasks. Useful for naively parallel tasks.",
                    "type": "array",
                    "items": {
                      "type": "string"
                    }
                  },
                  "jarFileUris": {
                    "description": "HCFS URIs of jar files to add to the CLASSPATHs of the Spark driver and tasks.",
                    "type": "array",
                    "items": {
                      "type": "string"
                    }
                  },
                  "loggingConfig": {
                    "type": "array",
                    "items": {
                      "type": "object",
                      "properties": {
                        "driverLogLevels": {
                          "description": "The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'",
                          "type": "object",
                          "additionalProperties": {
                            "type": "string"
                          }
                        }
                      }
                    }
                  },
                  "mainClass": {
                    "description": "The class containing the main method of the driver. Must be in a provided jar or jar that is already on the classpath. Conflicts with main_jar_file_uri",
                    "type": "string"
                  },
                  "mainJarFileUri": {
                    "description": "The HCFS URI of jar file containing the driver jar. Conflicts with main_class",
                    "type": "string"
                  },
                  "properties": {
                    "description": "A mapping of property names to values, used to configure Spark. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.",
                    "type": "object",
                    "additionalProperties": {
                      "type": "string"
                    }
                  }
                }
              }
            },
            "sparksqlConfig": {
              "type": "array",
              "items": {
                "type": "object",
                "properties": {
                  "jarFileUris": {
                    "description": "HCFS URIs of jar files to be added to the Spark CLASSPATH.",
                    "type": "array",
                    "items": {
                      "type": "string"
                    }
                  },
                  "loggingConfig": {
                    "type": "array",
                    "items": {
                      "type": "object",
                      "properties": {
                        "driverLogLevels": {
                          "description": "The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'",
                          "type": "object",
                          "additionalProperties": {
                            "type": "string"
                          }
                        }
                      }
                    }
                  },
                  "properties": {
                    "description": "A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Cloud Dataproc API may be overwritten.",
                    "type": "object",
                    "additionalProperties": {
                      "type": "string"
                    }
                  },
                  "queryFileUri": {
                    "description": "The HCFS URI of the script that contains SQL queries. Conflicts with query_list",
                    "type": "string"
                  },
                  "queryList": {
                    "description": "The list of SQL queries or statements to execute as part of the job. Conflicts with query_file_uri",
                    "type": "array",
                    "items": {
                      "type": "string"
                    }
                  },
                  "scriptVariables": {
                    "description": "Mapping of query variable names to values (equivalent to the Spark SQL command: SET name=\"value\";).",
                    "type": "object",
                    "additionalProperties": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        },
        "initProvider": {
          "description": "THIS IS AN ALPHA FIELD. Do not use it in production. It is not honored unless the relevant Crossplane feature flag is enabled, and may be changed or removed without notice. InitProvider holds the same fields as ForProvider, with the exception of Identifier and other resource reference fields. The fields that are in InitProvider are merged into ForProvider when the resource is created. The same fields are also added to the terraform ignore_changes hook, to avoid updating them after creation. This is useful for fields that are required on creation, but we do not desire to update them after creation, for example because of an external controller is managing them, like an autoscaler.",
          "type": "object",
          "properties": {
            "forceDelete": {
              "description": "By default, you can only delete inactive jobs within Dataproc. Setting this to true, and calling destroy, will ensure that the job is first cancelled before issuing the delete.",
              "type": "boolean"
            },
            "hadoopConfig": {
              "type": "array",
              "items": {
                "type": "object",
                "properties": {
                  "archiveUris": {
                    "description": "HCFS URIs of archives to be extracted in the working directory of .jar, .tar, .tar.gz, .tgz, and .zip.",
                    "type": "array",
                    "items": {
                      "type": "string"
                    }
                  },
                  "args": {
                    "description": "The arguments to pass to the driver. Do not include arguments, such as -libjars or -Dfoo=bar, that can be set as job properties, since a collision may occur that causes an incorrect job submission.",
                    "type": "array",
                    "items": {
                      "type": "string"
                    }
                  },
                  "fileUris": {
                    "description": "HCFS URIs of files to be copied to the working directory of Hadoop drivers and distributed tasks. Useful for naively parallel tasks.",
                    "type": "array",
                    "items": {
                      "type": "string"
                    }
                  },
                  "jarFileUris": {
                    "description": "HCFS URIs of jar files to add to the CLASSPATHs of the Spark driver and tasks.",
                    "type": "array",
                    "items": {
                      "type": "string"
                    }
                  },
                  "loggingConfig": {
                    "type": "array",
                    "items": {
                      "type": "object",
                      "properties": {
                        "driverLogLevels": {
                          "description": "The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'",
                          "type": "object",
                          "additionalProperties": {
                            "type": "string"
                          }
                        }
                      }
                    }
                  },
                  "mainClass": {
                    "description": "The name of the driver's main class. The jar file containing the class must be in the default CLASSPATH or specified in jar_file_uris. Conflicts with main_jar_file_uri",
                    "type": "string"
                  },
                  "mainJarFileUri": {
                    "description": "The HCFS URI of the jar file containing the main class. Examples: 'gs://foo-bucket/analytics-binaries/extract-useful-metrics-mr.jar' 'hdfs:/tmp/test-samples/custom-wordcount.jar' 'file:///home/usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar'. Conflicts with main_class",
                    "type": "string"
                  },
                  "properties": {
                    "description": "A mapping of property names to values, used to configure Hadoop. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site and classes in user code..",
                    "type": "object",
                    "additionalProperties": {
                      "type": "string"
                    }
                  }
                }
              }
            },
            "hiveConfig": {
              "type": "array",
              "items": {
                "type": "object",
                "properties": {
                  "continueOnFailure": {
                    "description": "Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries. Defaults to false.",
                    "type": "boolean"
                  },
                  "jarFileUris": {
                    "description": "HCFS URIs of jar files to add to the CLASSPATH of the Hive server and Hadoop MapReduce (MR) tasks. Can contain Hive SerDes and UDFs.",
                    "type": "array",
                    "items": {
                      "type": "string"
                    }
                  },
                  "properties": {
                    "description": "A mapping of property names and values, used to configure Hive. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml, /etc/hive/conf/hive-site.xml, and classes in user code..",
                    "type": "object",
                    "additionalProperties": {
                      "type": "string"
                    }
                  },
                  "queryFileUri": {
                    "description": "HCFS URI of file containing Hive script to execute as the job. Conflicts with query_list",
                    "type": "string"
                  },
                  "queryList": {
                    "description": "The list of Hive queries or statements to execute as part of the job. Conflicts with query_file_uri",
                    "type": "array",
                    "items": {
                      "type": "string"
                    }
                  },
                  "scriptVariables": {
                    "description": "Mapping of query variable names to values (equivalent to the Hive command: SET name=\"value\";).",
                    "type": "object",
                    "additionalProperties": {
                      "type": "string"
                    }
                  }
                }
              }
            },
            "labels": {
              "description": "The list of labels (key/value pairs) to add to the job.",
              "type": "object",
              "additionalProperties": {
                "type": "string"
              }
            },
            "pigConfig": {
              "type": "array",
              "items": {
                "type": "object",
                "properties": {
                  "continueOnFailure": {
                    "description": "Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries. Defaults to false.",
                    "type": "boolean"
                  },
                  "jarFileUris": {
                    "description": "HCFS URIs of jar files to add to the CLASSPATH of the Pig Client and Hadoop MapReduce (MR) tasks. Can contain Pig UDFs.",
                    "type": "array",
                    "items": {
                      "type": "string"
                    }
                  },
                  "loggingConfig": {
                    "type": "array",
                    "items": {
                      "type": "object",
                      "properties": {
                        "driverLogLevels": {
                          "description": "The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'",
                          "type": "object",
                          "additionalProperties": {
                            "type": "string"
                          }
                        }
                      }
                    }
                  },
                  "properties": {
                    "description": "A mapping of property names to values, used to configure Pig. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml, /etc/pig/conf/pig.properties, and classes in user code.",
                    "type": "object",
                    "additionalProperties": {
                      "type": "string"
                    }
                  },
                  "queryFileUri": {
                    "description": "HCFS URI of file containing Hive script to execute as the job. Conflicts with query_list",
                    "type": "string"
                  },
                  "queryList": {
                    "description": "The list of Hive queries or statements to execute as part of the job. Conflicts with query_file_uri",
                    "type": "array",
                    "items": {
                      "type": "string"
                    }
                  },
                  "scriptVariables": {
                    "description": "Mapping of query variable names to values (equivalent to the Pig command: name=[value]).",
                    "type": "object",
                    "additionalProperties": {
                      "type": "string"
                    }
                  }
                }
              }
            },
            "placement": {
              "type": "array",
              "items": {
                "type": "object"
              }
            },
            "prestoConfig": {
              "type": "array",
              "items": {
                "type": "object",
                "properties": {
                  "clientTags": {
                    "description": "Presto client tags to attach to this query.",
                    "type": "array",
                    "items": {
                      "type": "string"
                    }
                  },
                  "continueOnFailure": {
                    "description": "Whether to continue executing queries if a query fails. Setting to true can be useful when executing independent parallel queries. Defaults to false.",
                    "type": "boolean"
                  },
                  "loggingConfig": {
                    "type": "array",
                    "items": {
                      "type": "object",
                      "properties": {
                        "driverLogLevels": {
                          "description": "The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'",
                          "type": "object",
                          "additionalProperties": {
                            "type": "string"
                          }
                        }
                      }
                    }
                  },
                  "outputFormat": {
                    "description": "The format in which query output will be displayed. See the Presto documentation for supported output formats.",
                    "type": "string"
                  },
                  "properties": {
                    "description": "A mapping of property names to values. Used to set Presto session properties Equivalent to using the --session flag in the Presto CLI.",
                    "type": "object",
                    "additionalProperties": {
                      "type": "string"
                    }
                  },
                  "queryFileUri": {
                    "description": "The HCFS URI of the script that contains SQL queries. Conflicts with query_list",
                    "type": "string"
                  },
                  "queryList": {
                    "description": "The list of SQL queries or statements to execute as part of the job. Conflicts with query_file_uri",
                    "type": "array",
                    "items": {
                      "type": "string"
                    }
                  }
                }
              }
            },
            "project": {
              "description": "The project in which the cluster can be found and jobs subsequently run against. If it is not provided, the provider project is used.",
              "type": "string"
            },
            "pysparkConfig": {
              "type": "array",
              "items": {
                "type": "object",
                "properties": {
                  "archiveUris": {
                    "description": "HCFS URIs of archives to be extracted in the working directory of .jar, .tar, .tar.gz, .tgz, and .zip.",
                    "type": "array",
                    "items": {
                      "type": "string"
                    }
                  },
                  "args": {
                    "description": "The arguments to pass to the driver.",
                    "type": "array",
                    "items": {
                      "type": "string"
                    }
                  },
                  "fileUris": {
                    "description": "HCFS URIs of files to be copied to the working directory of Python drivers and distributed tasks. Useful for naively parallel tasks.",
                    "type": "array",
                    "items": {
                      "type": "string"
                    }
                  },
                  "jarFileUris": {
                    "description": "HCFS URIs of jar files to add to the CLASSPATHs of the Python driver and tasks.",
                    "type": "array",
                    "items": {
                      "type": "string"
                    }
                  },
                  "loggingConfig": {
                    "type": "array",
                    "items": {
                      "type": "object",
                      "properties": {
                        "driverLogLevels": {
                          "description": "The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'",
                          "type": "object",
                          "additionalProperties": {
                            "type": "string"
                          }
                        }
                      }
                    }
                  },
                  "mainPythonFileUri": {
                    "description": "The HCFS URI of the main Python file to use as the driver. Must be a .py file.",
                    "type": "string"
                  },
                  "properties": {
                    "description": "A mapping of property names to values, used to configure PySpark. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.",
                    "type": "object",
                    "additionalProperties": {
                      "type": "string"
                    }
                  },
                  "pythonFileUris": {
                    "description": "HCFS file URIs of Python files to pass to the PySpark framework. Supported file types: .py, .egg, and .zip.",
                    "type": "array",
                    "items": {
                      "type": "string"
                    }
                  }
                }
              }
            },
            "reference": {
              "type": "array",
              "items": {
                "type": "object",
                "properties": {
                  "jobId": {
                    "type": "string"
                  }
                }
              }
            },
            "scheduling": {
              "type": "array",
              "items": {
                "type": "object",
                "properties": {
                  "maxFailuresPerHour": {
                    "description": "Maximum number of times per hour a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed.",
                    "type": "number"
                  },
                  "maxFailuresTotal": {
                    "description": "Maximum number of times in total a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed.",
                    "type": "number"
                  }
                }
              }
            },
            "sparkConfig": {
              "type": "array",
              "items": {
                "type": "object",
                "properties": {
                  "archiveUris": {
                    "description": "HCFS URIs of archives to be extracted in the working directory of .jar, .tar, .tar.gz, .tgz, and .zip.",
                    "type": "array",
                    "items": {
                      "type": "string"
                    }
                  },
                  "args": {
                    "description": "The arguments to pass to the driver.",
                    "type": "array",
                    "items": {
                      "type": "string"
                    }
                  },
                  "fileUris": {
                    "description": "HCFS URIs of files to be copied to the working directory of Spark drivers and distributed tasks. Useful for naively parallel tasks.",
                    "type": "array",
                    "items": {
                      "type": "string"
                    }
                  },
                  "jarFileUris": {
                    "description": "HCFS URIs of jar files to add to the CLASSPATHs of the Spark driver and tasks.",
                    "type": "array",
                    "items": {
                      "type": "string"
                    }
                  },
                  "loggingConfig": {
                    "type": "array",
                    "items": {
                      "type": "object",
                      "properties": {
                        "driverLogLevels": {
                          "description": "The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'",
                          "type": "object",
                          "additionalProperties": {
                            "type": "string"
                          }
                        }
                      }
                    }
                  },
                  "mainClass": {
                    "description": "The class containing the main method of the driver. Must be in a provided jar or jar that is already on the classpath. Conflicts with main_jar_file_uri",
                    "type": "string"
                  },
                  "mainJarFileUri": {
                    "description": "The HCFS URI of jar file containing the driver jar. Conflicts with main_class",
                    "type": "string"
                  },
                  "properties": {
                    "description": "A mapping of property names to values, used to configure Spark. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.",
                    "type": "object",
                    "additionalProperties": {
                      "type": "string"
                    }
                  }
                }
              }
            },
            "sparksqlConfig": {
              "type": "array",
              "items": {
                "type": "object",
                "properties": {
                  "jarFileUris": {
                    "description": "HCFS URIs of jar files to be added to the Spark CLASSPATH.",
                    "type": "array",
                    "items": {
                      "type": "string"
                    }
                  },
                  "loggingConfig": {
                    "type": "array",
                    "items": {
                      "type": "object",
                      "properties": {
                        "driverLogLevels": {
                          "description": "The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'",
                          "type": "object",
                          "additionalProperties": {
                            "type": "string"
                          }
                        }
                      }
                    }
                  },
                  "properties": {
                    "description": "A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Cloud Dataproc API may be overwritten.",
                    "type": "object",
                    "additionalProperties": {
                      "type": "string"
                    }
                  },
                  "queryFileUri": {
                    "description": "The HCFS URI of the script that contains SQL queries. Conflicts with query_list",
                    "type": "string"
                  },
                  "queryList": {
                    "description": "The list of SQL queries or statements to execute as part of the job. Conflicts with query_file_uri",
                    "type": "array",
                    "items": {
                      "type": "string"
                    }
                  },
                  "scriptVariables": {
                    "description": "Mapping of query variable names to values (equivalent to the Spark SQL command: SET name=\"value\";).",
                    "type": "object",
                    "additionalProperties": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        },
        "managementPolicies": {
          "description": "THIS IS AN ALPHA FIELD. Do not use it in production. It is not honored unless the relevant Crossplane feature flag is enabled, and may be changed or removed without notice. ManagementPolicies specify the array of actions Crossplane is allowed to take on the managed and external resources. This field is planned to replace the DeletionPolicy field in a future release. Currently, both could be set independently and non-default values would be honored if the feature flag is enabled. If both are custom, the DeletionPolicy field will be ignored. See the design doc for more information: https://github.com/crossplane/crossplane/blob/499895a25d1a1a0ba1604944ef98ac7a1a71f197/design/design-doc-observe-only-resources.md?plain=1#L223 and this one: https://github.com/crossplane/crossplane/blob/444267e84783136daa93568b364a5f01228cacbe/design/one-pager-ignore-changes.md",
          "type": "array",
          "default": [
            "*"
          ],
          "items": {
            "description": "A ManagementAction represents an action that the Crossplane controllers can take on an external resource.",
            "type": "string",
            "enum": [
              "Observe",
              "Create",
              "Update",
              "Delete",
              "LateInitialize",
              "*"
            ]
          }
        },
        "providerConfigRef": {
          "description": "ProviderConfigReference specifies how the provider that will be used to create, observe, update, and delete this managed resource should be configured.",
          "type": "object",
          "default": {
            "name": "default"
          },
          "required": [
            "name"
          ],
          "properties": {
            "name": {
              "description": "Name of the referenced object.",
              "type": "string"
            },
            "policy": {
              "description": "Policies for referencing.",
              "type": "object",
              "properties": {
                "resolution": {
                  "description": "Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.",
                  "type": "string",
                  "default": "Required",
                  "enum": [
                    "Required",
                    "Optional"
                  ]
                },
                "resolve": {
                  "description": "Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.",
                  "type": "string",
                  "enum": [
                    "Always",
                    "IfNotPresent"
                  ]
                }
              }
            }
          }
        },
        "publishConnectionDetailsTo": {
          "description": "PublishConnectionDetailsTo specifies the connection secret config which contains a name, metadata and a reference to secret store config to which any connection details for this managed resource should be written. Connection details frequently include the endpoint, username, and password required to connect to the managed resource.",
          "type": "object",
          "required": [
            "name"
          ],
          "properties": {
            "configRef": {
              "description": "SecretStoreConfigRef specifies which secret store config should be used for this ConnectionSecret.",
              "type": "object",
              "default": {
                "name": "default"
              },
              "required": [
                "name"
              ],
              "properties": {
                "name": {
                  "description": "Name of the referenced object.",
                  "type": "string"
                },
                "policy": {
                  "description": "Policies for referencing.",
                  "type": "object",
                  "properties": {
                    "resolution": {
                      "description": "Resolution specifies whether resolution of this reference is required. The default is 'Required', which means the reconcile will fail if the reference cannot be resolved. 'Optional' means this reference will be a no-op if it cannot be resolved.",
                      "type": "string",
                      "default": "Required",
                      "enum": [
                        "Required",
                        "Optional"
                      ]
                    },
                    "resolve": {
                      "description": "Resolve specifies when this reference should be resolved. The default is 'IfNotPresent', which will attempt to resolve the reference only when the corresponding field is not present. Use 'Always' to resolve the reference on every reconcile.",
                      "type": "string",
                      "enum": [
                        "Always",
                        "IfNotPresent"
                      ]
                    }
                  }
                }
              }
            },
            "metadata": {
              "description": "Metadata is the metadata for connection secret.",
              "type": "object",
              "properties": {
                "annotations": {
                  "description": "Annotations are the annotations to be added to connection secret. - For Kubernetes secrets, this will be used as \"metadata.annotations\". - It is up to Secret Store implementation for others store types.",
                  "type": "object",
                  "additionalProperties": {
                    "type": "string"
                  }
                },
                "labels": {
                  "description": "Labels are the labels/tags to be added to connection secret. - For Kubernetes secrets, this will be used as \"metadata.labels\". - It is up to Secret Store implementation for others store types.",
                  "type": "object",
                  "additionalProperties": {
                    "type": "string"
                  }
                },
                "type": {
                  "description": "Type is the SecretType for the connection secret. - Only valid for Kubernetes Secret Stores.",
                  "type": "string"
                }
              }
            },
            "name": {
              "description": "Name is the name of the connection secret.",
              "type": "string"
            }
          }
        },
        "writeConnectionSecretToRef": {
          "description": "WriteConnectionSecretToReference specifies the namespace and name of a Secret to which any connection details for this managed resource should be written. Connection details frequently include the endpoint, username, and password required to connect to the managed resource. This field is planned to be replaced in a future release in favor of PublishConnectionDetailsTo. Currently, both could be set independently and connection details would be published to both without affecting each other.",
          "type": "object",
          "required": [
            "name",
            "namespace"
          ],
          "properties": {
            "name": {
              "description": "Name of the secret.",
              "type": "string"
            },
            "namespace": {
              "description": "Namespace of the secret.",
              "type": "string"
            }
          }
        }
      },
      "x-kubernetes-validations": [
        {
          "rule": "!('*' in self.managementPolicies || 'Create' in self.managementPolicies || 'Update' in self.managementPolicies) || has(self.forProvider.placement) || (has(self.initProvider) \u0026\u0026 has(self.initProvider.placement))",
          "message": "spec.forProvider.placement is a required parameter"
        }
      ]
    }
  }
}